{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Bank Marketing Prediction — ML Coursework\n",
                "\n",
                "**Objective:** Predict whether a client will subscribe to a term deposit (`y`: yes/no) based on a Portuguese bank's telemarketing campaign data.\n",
                "\n",
                "**Dataset:** [UCI Bank Marketing Dataset](https://archive.ics.uci.edu/dataset/222/bank+marketing) — 41,188 records, 20 input features + 1 target.\n",
                "\n",
                "---\n",
                "\n",
                "## Table of Contents\n",
                "1. [Data Loading & Exploration](#1)\n",
                "2. [Task A — Preprocessing](#2)\n",
                "   - 2.1 Missing Values & Outliers\n",
                "   - 2.2 Feature Encoding\n",
                "   - 2.3 Scaling / Standardisation\n",
                "3. [Task B — Model Building (LR & SVM)](#3)\n",
                "4. [Task C — Discussion & Comparison](#4)\n",
                "5. [Model Export for Deployment](#5)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 0. Imports & Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Core\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Visualisation\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "sns.set_theme(style='whitegrid', palette='muted', font_scale=1.1)\n",
                "%matplotlib inline\n",
                "\n",
                "# Preprocessing & Modelling\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
                "from sklearn.compose import ColumnTransformer\n",
                "from sklearn.pipeline import Pipeline\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.svm import SVC\n",
                "from sklearn.metrics import (\n",
                "    classification_report, confusion_matrix, ConfusionMatrixDisplay,\n",
                "    roc_auc_score, roc_curve, accuracy_score, f1_score,\n",
                "    precision_score, recall_score\n",
                ")\n",
                "\n",
                "# Model persistence\n",
                "import joblib\n",
                "import json\n",
                "import os\n",
                "\n",
                "RANDOM_STATE = 42\n",
                "TEST_SIZE = 0.2\n",
                "\n",
                "# Create output directories\n",
                "os.makedirs('figures', exist_ok=True)\n",
                "os.makedirs('model', exist_ok=True)\n",
                "\n",
                "print('All imports loaded successfully.')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<a id='1'></a>\n",
                "## 1. Data Loading & Exploration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load dataset (semicolon-separated)\n",
                "df = pd.read_csv('data/bank-additional-full.csv', sep=';')\n",
                "print(f'Dataset shape: {df.shape}')\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Basic info\n",
                "df.info()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Statistical summary\n",
                "df.describe(include='all').T"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Target variable distribution\n",
                "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
                "\n",
                "# Count plot\n",
                "sns.countplot(x='y', data=df, ax=axes[0], palette='viridis')\n",
                "axes[0].set_title('Target Variable Distribution (Count)')\n",
                "axes[0].set_xlabel('Subscribed (y)')\n",
                "axes[0].set_ylabel('Count')\n",
                "\n",
                "# Percentage\n",
                "df['y'].value_counts(normalize=True).plot.pie(\n",
                "    autopct='%1.1f%%', ax=axes[1], colors=['#3498db', '#e74c3c'],\n",
                "    startangle=90, explode=[0, 0.05]\n",
                ")\n",
                "axes[1].set_title('Target Variable Distribution (%)')\n",
                "axes[1].set_ylabel('')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('figures/target_distribution.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "no_count = df['y'].value_counts()['no']\n",
                "yes_count = df['y'].value_counts()['yes']\n",
                "print(f'\\nClass imbalance ratio: {no_count / yes_count:.2f}:1 (no:yes)')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<a id='2'></a>\n",
                "## 2. Task A — Preprocessing\n",
                "\n",
                "### 2.1 Missing Values & Outliers"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check for explicit missing values\n",
                "print('=== Null values per column ===')\n",
                "print(df.isnull().sum())\n",
                "print(f'\\nTotal nulls: {df.isnull().sum().sum()}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check for 'unknown' values (implicit missing values in this dataset)\n",
                "print('=== \"unknown\" counts per categorical column ===')\n",
                "cat_cols = df.select_dtypes(include='object').columns\n",
                "for col in cat_cols:\n",
                "    unknown_count = (df[col] == 'unknown').sum()\n",
                "    if unknown_count > 0:\n",
                "        pct = unknown_count / len(df) * 100\n",
                "        print(f'  {col:15s}: {unknown_count:6d} ({pct:.2f}%)')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ASSUMPTION: We treat 'unknown' as a valid category rather than imputing.\n",
                "# Rationale: \n",
                "#   - 'unknown' in default (8,597 / 20.9%) is too large to drop.\n",
                "#   - The bank recorded 'unknown' deliberately; it carries information.\n",
                "#   - The OneHotEncoder will create a separate indicator for 'unknown'.\n",
                "\n",
                "# Check for duplicates\n",
                "n_dup = df.duplicated().sum()\n",
                "print(f'Duplicate rows: {n_dup}')\n",
                "if n_dup > 0:\n",
                "    df = df.drop_duplicates()\n",
                "    print(f'After removing duplicates: {df.shape}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Outlier analysis for numeric columns\n",
                "numeric_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
                "print(f'Numeric columns: {numeric_cols}\\n')\n",
                "\n",
                "fig, axes = plt.subplots(3, 4, figsize=(18, 12))\n",
                "axes = axes.flatten()\n",
                "\n",
                "for i, col in enumerate(numeric_cols):\n",
                "    sns.boxplot(y=df[col], ax=axes[i], color='#5dade2')\n",
                "    axes[i].set_title(col, fontsize=11)\n",
                "\n",
                "# Hide unused subplots\n",
                "for j in range(len(numeric_cols), len(axes)):\n",
                "    axes[j].set_visible(False)\n",
                "\n",
                "plt.suptitle('Box Plots — Outlier Detection for Numeric Features', fontsize=14, y=1.01)\n",
                "plt.tight_layout()\n",
                "plt.savefig('figures/outlier_boxplots.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Outlier summary using IQR method\n",
                "print('=== Outlier Summary (IQR Method) ===')\n",
                "for col in numeric_cols:\n",
                "    Q1 = df[col].quantile(0.25)\n",
                "    Q3 = df[col].quantile(0.75)\n",
                "    IQR = Q3 - Q1\n",
                "    lower = Q1 - 1.5 * IQR\n",
                "    upper = Q3 + 1.5 * IQR\n",
                "    outliers = ((df[col] < lower) | (df[col] > upper)).sum()\n",
                "    pct = outliers / len(df) * 100\n",
                "    if outliers > 0:\n",
                "        print(f'  {col:20s}: {outliers:5d} outliers ({pct:.2f}%)')\n",
                "\n",
                "# ASSUMPTION: We do NOT remove outliers.\n",
                "# Rationale: \n",
                "#   - Outliers in 'campaign', 'age', etc. represent real client behaviour.\n",
                "#   - SVM and LR are used with StandardScaler which mitigates extreme values.\n",
                "#   - Removing outliers would lose valuable data from an already imbalanced dataset.\n",
                "print('\\n=> Decision: Retain outliers. StandardScaler will mitigate their influence.')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.2 Feature Engineering & Duration Handling\n",
                "\n",
                "**IMPORTANT — Duration Leakage Note:**\n",
                "\n",
                "> `duration` is the last contact duration in seconds. It highly affects the output target. However, `duration` is **not known before a call is performed**, and after the call ends, `y` is already known. Therefore, `duration` should only be included for **benchmark purposes** and must be **discarded for a realistic predictive model**.\n",
                "\n",
                "We will build **two versions**:\n",
                "1. **Benchmark model** — includes `duration` (to show its predictive power)\n",
                "2. **Realistic model** — excludes `duration` (deployed model)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Encode target variable\n",
                "le = LabelEncoder()\n",
                "df['y_encoded'] = le.fit_transform(df['y'])  # no=0, yes=1\n",
                "target_map = dict(zip(le.classes_, le.transform(le.classes_)))\n",
                "print(f'Target encoding: {target_map}')\n",
                "\n",
                "# Define feature groups\n",
                "# --- Features for DEPLOYMENT (realistic model — no duration, no campaign-contact features) ---\n",
                "DEPLOYMENT_FEATURES = ['age', 'job', 'marital', 'education', 'default', 'housing', 'loan']\n",
                "\n",
                "# --- All features for BENCHMARK model ---\n",
                "ALL_FEATURES = [c for c in df.columns if c not in ['y', 'y_encoded']]\n",
                "\n",
                "# --- Realistic model features (exclude duration but keep other campaign/economic features) ---\n",
                "REALISTIC_FEATURES = [c for c in ALL_FEATURES if c != 'duration']\n",
                "\n",
                "print(f'\\nDeployment features ({len(DEPLOYMENT_FEATURES)}): {DEPLOYMENT_FEATURES}')\n",
                "print(f'Realistic features  ({len(REALISTIC_FEATURES)}): {REALISTIC_FEATURES}')\n",
                "print(f'All features        ({len(ALL_FEATURES)}): {ALL_FEATURES}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Correlation heatmap for numeric features\n",
                "plt.figure(figsize=(14, 10))\n",
                "corr_cols = numeric_cols + ['y_encoded']\n",
                "corr_matrix = df[corr_cols].corr()\n",
                "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
                "sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f',\n",
                "            cmap='RdBu_r', center=0, vmin=-1, vmax=1,\n",
                "            linewidths=0.5, square=True)\n",
                "plt.title('Correlation Matrix — Numeric Features + Target', fontsize=14)\n",
                "plt.tight_layout()\n",
                "plt.savefig('figures/correlation_heatmap.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print('\\n=== Correlation with target (y_encoded) ===')\n",
                "target_corr = corr_matrix['y_encoded'].drop('y_encoded').sort_values(ascending=False)\n",
                "print(target_corr)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.3 Scaling / Standardisation\n",
                "\n",
                "We use `sklearn.pipeline.Pipeline` + `ColumnTransformer` to ensure preprocessing is **identical** during training and deployment."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def build_preprocessor(feature_list):\n",
                "    \"\"\"\n",
                "    Build a ColumnTransformer that:\n",
                "      - StandardScaler on numeric features\n",
                "      - OneHotEncoder on categorical features\n",
                "    \"\"\"\n",
                "    num_features = [f for f in feature_list if df[f].dtype in ['int64', 'float64']]\n",
                "    cat_features = [f for f in feature_list if df[f].dtype == 'object']\n",
                "\n",
                "    preprocessor = ColumnTransformer(\n",
                "        transformers=[\n",
                "            ('num', StandardScaler(), num_features),\n",
                "            ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_features)\n",
                "        ],\n",
                "        remainder='drop'\n",
                "    )\n",
                "    return preprocessor, num_features, cat_features\n",
                "\n",
                "print('Preprocessor builder ready.')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Demonstrate the effect of scaling on the DEPLOYMENT feature set\n",
                "deploy_preprocessor, deploy_num, deploy_cat = build_preprocessor(DEPLOYMENT_FEATURES)\n",
                "\n",
                "print(f'Numeric features to scale: {deploy_num}')\n",
                "print(f'Categorical features to encode: {deploy_cat}')\n",
                "\n",
                "# Show before/after scaling for numeric feature 'age'\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Before scaling\n",
                "axes[0].hist(df['age'], bins=40, color='#e74c3c', edgecolor='white', alpha=0.8)\n",
                "axes[0].set_title('Age — Before Scaling (Raw)', fontsize=13)\n",
                "axes[0].set_xlabel('Age')\n",
                "axes[0].set_ylabel('Frequency')\n",
                "age_mean = df['age'].mean()\n",
                "age_std = df['age'].std()\n",
                "axes[0].axvline(age_mean, color='black', linestyle='--', label=f'Mean={age_mean:.1f}')\n",
                "axes[0].axvline(age_std, color='gray', linestyle=':', label=f'Std={age_std:.1f}')\n",
                "axes[0].legend()\n",
                "\n",
                "# After scaling\n",
                "scaler_demo = StandardScaler()\n",
                "age_scaled = scaler_demo.fit_transform(df[['age']])\n",
                "axes[1].hist(age_scaled, bins=40, color='#2ecc71', edgecolor='white', alpha=0.8)\n",
                "axes[1].set_title('Age — After StandardScaler', fontsize=13)\n",
                "axes[1].set_xlabel('Scaled Age (z-score)')\n",
                "axes[1].set_ylabel('Frequency')\n",
                "scaled_mean = age_scaled.mean()\n",
                "scaled_std = age_scaled.std()\n",
                "axes[1].axvline(scaled_mean, color='black', linestyle='--', label=f'Mean={scaled_mean:.4f}')\n",
                "axes[1].axvline(scaled_std, color='gray', linestyle=':', label=f'Std={scaled_std:.4f}')\n",
                "axes[1].legend()\n",
                "\n",
                "plt.suptitle('Effect of StandardScaler on Numeric Feature', fontsize=15, y=1.02)\n",
                "plt.tight_layout()\n",
                "plt.savefig('figures/scaling_effect_age.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Show scaling effect across ALL numeric features (before vs after)\n",
                "real_preprocessor, real_num, real_cat = build_preprocessor(REALISTIC_FEATURES)\n",
                "\n",
                "fig, axes = plt.subplots(2, 1, figsize=(16, 8))\n",
                "\n",
                "# Before\n",
                "df[real_num].boxplot(ax=axes[0], vert=False, patch_artist=True,\n",
                "                     boxprops=dict(facecolor='#e74c3c', alpha=0.6))\n",
                "axes[0].set_title('Numeric Features — BEFORE Scaling', fontsize=13)\n",
                "axes[0].set_xlabel('Raw Value')\n",
                "\n",
                "# After\n",
                "scaler_all = StandardScaler()\n",
                "df_scaled = pd.DataFrame(scaler_all.fit_transform(df[real_num]), columns=real_num)\n",
                "df_scaled.boxplot(ax=axes[1], vert=False, patch_artist=True,\n",
                "                  boxprops=dict(facecolor='#2ecc71', alpha=0.6))\n",
                "axes[1].set_title('Numeric Features — AFTER StandardScaler', fontsize=13)\n",
                "axes[1].set_xlabel('Standardised Value (z-score)')\n",
                "\n",
                "plt.suptitle('Effect of Scaling / Standardisation on All Numeric Features', fontsize=15, y=1.02)\n",
                "plt.tight_layout()\n",
                "plt.savefig('figures/scaling_effect_all.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print('\\n=== Before Scaling ===')\n",
                "print(df[real_num].describe().loc[['mean', 'std', 'min', 'max']].T)\n",
                "print('\\n=== After Scaling ===')\n",
                "print(df_scaled.describe().loc[['mean', 'std', 'min', 'max']].T)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<a id='3'></a>\n",
                "## 3. Task B — Model Building\n",
                "\n",
                "**Train/Test Split:** 80/20 with stratification to preserve class distribution.\n",
                "\n",
                "We build two pipelines for each model type:\n",
                "1. **Realistic model** (excludes `duration`) — the deployable model\n",
                "2. **Benchmark model** (includes `duration`) — to show duration's influence\n",
                "\n",
                "We also build a **deployment model** using only the 7 form fields."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare target\n",
                "y = df['y_encoded']\n",
                "\n",
                "# =============================\n",
                "# SPLIT 1: Realistic features\n",
                "# =============================\n",
                "X_real = df[REALISTIC_FEATURES]\n",
                "X_real_train, X_real_test, y_real_train, y_real_test = train_test_split(\n",
                "    X_real, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n",
                ")\n",
                "print(f'Realistic split — Train: {X_real_train.shape}, Test: {X_real_test.shape}')\n",
                "\n",
                "# =============================\n",
                "# SPLIT 2: All features (benchmark)\n",
                "# =============================\n",
                "X_all = df[ALL_FEATURES]\n",
                "X_all_train, X_all_test, y_all_train, y_all_test = train_test_split(\n",
                "    X_all, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n",
                ")\n",
                "print(f'Benchmark split  — Train: {X_all_train.shape}, Test: {X_all_test.shape}')\n",
                "\n",
                "# =============================\n",
                "# SPLIT 3: Deployment features (7 form fields only)\n",
                "# =============================\n",
                "X_deploy = df[DEPLOYMENT_FEATURES]\n",
                "X_deploy_train, X_deploy_test, y_deploy_train, y_deploy_test = train_test_split(\n",
                "    X_deploy, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n",
                ")\n",
                "print(f'Deployment split — Train: {X_deploy_train.shape}, Test: {X_deploy_test.shape}')\n",
                "\n",
                "train_dist = np.bincount(y_real_train)\n",
                "test_dist = np.bincount(y_real_test)\n",
                "print(f'\\nTarget distribution (train): {train_dist} | (test): {test_dist}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def build_pipeline(features, model, model_name):\n",
                "    \"\"\"Build a complete sklearn Pipeline: preprocessor -> model.\"\"\"\n",
                "    preprocessor, num_f, cat_f = build_preprocessor(features)\n",
                "    pipe = Pipeline([\n",
                "        ('preprocessor', preprocessor),\n",
                "        ('classifier', model)\n",
                "    ])\n",
                "    return pipe\n",
                "\n",
                "def evaluate_model(pipe, X_test, y_test, model_name, dataset_label):\n",
                "    \"\"\"Evaluate a trained pipeline and return metrics dict.\"\"\"\n",
                "    y_pred = pipe.predict(X_test)\n",
                "    \n",
                "    # Probabilities for ROC-AUC (if available)\n",
                "    try:\n",
                "        y_prob = pipe.predict_proba(X_test)[:, 1]\n",
                "        roc_auc = roc_auc_score(y_test, y_prob)\n",
                "    except AttributeError:\n",
                "        y_prob = pipe.decision_function(X_test)\n",
                "        roc_auc = roc_auc_score(y_test, y_prob)\n",
                "    \n",
                "    acc = accuracy_score(y_test, y_pred)\n",
                "    prec = precision_score(y_test, y_pred)\n",
                "    rec = recall_score(y_test, y_pred)\n",
                "    f1 = f1_score(y_test, y_pred)\n",
                "    \n",
                "    sep = '=' * 60\n",
                "    print(f'\\n{sep}')\n",
                "    print(f'{model_name} — {dataset_label}')\n",
                "    print(f'{sep}')\n",
                "    print(f'Accuracy:  {acc:.4f}')\n",
                "    print(f'Precision: {prec:.4f}')\n",
                "    print(f'Recall:    {rec:.4f}')\n",
                "    print(f'F1 Score:  {f1:.4f}')\n",
                "    print(f'ROC-AUC:   {roc_auc:.4f}')\n",
                "    print('\\nClassification Report:')\n",
                "    print(classification_report(y_test, y_pred, target_names=['No', 'Yes']))\n",
                "    \n",
                "    return {\n",
                "        'model': model_name, 'dataset': dataset_label,\n",
                "        'accuracy': acc, 'precision': prec, 'recall': rec,\n",
                "        'f1': f1, 'roc_auc': roc_auc,\n",
                "        'y_pred': y_pred, 'y_prob': y_prob\n",
                "    }\n",
                "\n",
                "print('Pipeline builder & evaluator ready.')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.1 Logistic Regression"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- Logistic Regression: Realistic (no duration) ---\n",
                "lr_real_pipe = build_pipeline(\n",
                "    REALISTIC_FEATURES,\n",
                "    LogisticRegression(max_iter=1000, class_weight='balanced', random_state=RANDOM_STATE),\n",
                "    'LR'\n",
                ")\n",
                "lr_real_pipe.fit(X_real_train, y_real_train)\n",
                "lr_real_results = evaluate_model(lr_real_pipe, X_real_test, y_real_test, 'Logistic Regression', 'Realistic (no duration)')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- Logistic Regression: Benchmark (with duration) ---\n",
                "lr_bench_pipe = build_pipeline(\n",
                "    ALL_FEATURES,\n",
                "    LogisticRegression(max_iter=1000, class_weight='balanced', random_state=RANDOM_STATE),\n",
                "    'LR'\n",
                ")\n",
                "lr_bench_pipe.fit(X_all_train, y_all_train)\n",
                "lr_bench_results = evaluate_model(lr_bench_pipe, X_all_test, y_all_test, 'Logistic Regression', 'Benchmark (with duration)')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.2 Support Vector Machine (SVM)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- SVM: Realistic (no duration) ---\n",
                "# probability=True to enable predict_proba for ROC-AUC\n",
                "svm_real_pipe = build_pipeline(\n",
                "    REALISTIC_FEATURES,\n",
                "    SVC(kernel='rbf', class_weight='balanced', probability=True, random_state=RANDOM_STATE, C=1.0),\n",
                "    'SVM'\n",
                ")\n",
                "svm_real_pipe.fit(X_real_train, y_real_train)\n",
                "svm_real_results = evaluate_model(svm_real_pipe, X_real_test, y_real_test, 'SVM (RBF)', 'Realistic (no duration)')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- SVM: Benchmark (with duration) ---\n",
                "svm_bench_pipe = build_pipeline(\n",
                "    ALL_FEATURES,\n",
                "    SVC(kernel='rbf', class_weight='balanced', probability=True, random_state=RANDOM_STATE, C=1.0),\n",
                "    'SVM'\n",
                ")\n",
                "svm_bench_pipe.fit(X_all_train, y_all_train)\n",
                "svm_bench_results = evaluate_model(svm_bench_pipe, X_all_test, y_all_test, 'SVM (RBF)', 'Benchmark (with duration)')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.3 Deployment Model (7 Features Only)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train LR on deployment features (7 form fields)\n",
                "lr_deploy_pipe = build_pipeline(\n",
                "    DEPLOYMENT_FEATURES,\n",
                "    LogisticRegression(max_iter=1000, class_weight='balanced', random_state=RANDOM_STATE),\n",
                "    'LR'\n",
                ")\n",
                "lr_deploy_pipe.fit(X_deploy_train, y_deploy_train)\n",
                "lr_deploy_results = evaluate_model(lr_deploy_pipe, X_deploy_test, y_deploy_test, 'Logistic Regression', 'Deployment (7 features)')\n",
                "\n",
                "# Train SVM on deployment features (7 form fields)\n",
                "svm_deploy_pipe = build_pipeline(\n",
                "    DEPLOYMENT_FEATURES,\n",
                "    SVC(kernel='rbf', class_weight='balanced', probability=True, random_state=RANDOM_STATE, C=1.0),\n",
                "    'SVM'\n",
                ")\n",
                "svm_deploy_pipe.fit(X_deploy_train, y_deploy_train)\n",
                "svm_deploy_results = evaluate_model(svm_deploy_pipe, X_deploy_test, y_deploy_test, 'SVM (RBF)', 'Deployment (7 features)')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.4 Visualisation — Confusion Matrices & ROC Curves"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Confusion Matrices\n",
                "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
                "\n",
                "results_list = [\n",
                "    (lr_real_results, y_real_test, 'LR — Realistic'),\n",
                "    (lr_bench_results, y_all_test, 'LR — Benchmark'),\n",
                "    (lr_deploy_results, y_deploy_test, 'LR — Deployment'),\n",
                "    (svm_real_results, y_real_test, 'SVM — Realistic'),\n",
                "    (svm_bench_results, y_all_test, 'SVM — Benchmark'),\n",
                "    (svm_deploy_results, y_deploy_test, 'SVM — Deployment'),\n",
                "]\n",
                "\n",
                "for idx, (res, y_t, title) in enumerate(results_list):\n",
                "    ax = axes[idx // 3, idx % 3]\n",
                "    cm = confusion_matrix(y_t, res['y_pred'])\n",
                "    ConfusionMatrixDisplay(cm, display_labels=['No', 'Yes']).plot(ax=ax, cmap='Blues', colorbar=False)\n",
                "    ax.set_title(title, fontsize=12)\n",
                "\n",
                "plt.suptitle('Confusion Matrices — All Model Variants', fontsize=15, y=1.02)\n",
                "plt.tight_layout()\n",
                "plt.savefig('figures/confusion_matrices.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ROC Curves\n",
                "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
                "\n",
                "roc_groups = [\n",
                "    ('Realistic (no duration)', lr_real_results, svm_real_results, y_real_test),\n",
                "    ('Benchmark (with duration)', lr_bench_results, svm_bench_results, y_all_test),\n",
                "    ('Deployment (7 features)', lr_deploy_results, svm_deploy_results, y_deploy_test),\n",
                "]\n",
                "\n",
                "for idx, (label, lr_res, svm_res, y_t) in enumerate(roc_groups):\n",
                "    ax = axes[idx]\n",
                "    \n",
                "    # LR\n",
                "    fpr_lr, tpr_lr, _ = roc_curve(y_t, lr_res['y_prob'])\n",
                "    lr_auc = lr_res['roc_auc']\n",
                "    ax.plot(fpr_lr, tpr_lr, label=f'LR (AUC={lr_auc:.3f})', linewidth=2)\n",
                "    \n",
                "    # SVM\n",
                "    fpr_svm, tpr_svm, _ = roc_curve(y_t, svm_res['y_prob'])\n",
                "    svm_auc = svm_res['roc_auc']\n",
                "    ax.plot(fpr_svm, tpr_svm, label=f'SVM (AUC={svm_auc:.3f})', linewidth=2)\n",
                "    \n",
                "    # Diagonal\n",
                "    ax.plot([0, 1], [0, 1], 'k--', alpha=0.4, label='Random')\n",
                "    \n",
                "    ax.set_title(label, fontsize=12)\n",
                "    ax.set_xlabel('False Positive Rate')\n",
                "    ax.set_ylabel('True Positive Rate')\n",
                "    ax.legend(fontsize=10)\n",
                "    ax.grid(alpha=0.3)\n",
                "\n",
                "plt.suptitle('ROC Curves — LR vs SVM', fontsize=15, y=1.02)\n",
                "plt.tight_layout()\n",
                "plt.savefig('figures/roc_curves.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<a id='4'></a>\n",
                "## 4. Task C — Discussion & Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Summary comparison table\n",
                "all_results = [lr_real_results, lr_bench_results, lr_deploy_results,\n",
                "               svm_real_results, svm_bench_results, svm_deploy_results]\n",
                "\n",
                "summary_rows = []\n",
                "for r in all_results:\n",
                "    summary_rows.append({\n",
                "        'Model': r['model'],\n",
                "        'Dataset': r['dataset'],\n",
                "        'Accuracy': round(r['accuracy'], 4),\n",
                "        'Precision': round(r['precision'], 4),\n",
                "        'Recall': round(r['recall'], 4),\n",
                "        'F1': round(r['f1'], 4),\n",
                "        'ROC-AUC': round(r['roc_auc'], 4)\n",
                "    })\n",
                "\n",
                "summary_df = pd.DataFrame(summary_rows)\n",
                "\n",
                "print('\\n' + '=' * 60)\n",
                "print('           MODEL COMPARISON SUMMARY')\n",
                "print('=' * 60)\n",
                "display(summary_df.style.set_properties(**{'text-align': 'center'}))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Discussion\n",
                "\n",
                "#### 1. Duration Leakage Impact\n",
                "- **Benchmark models** (with `duration`) achieve significantly higher ROC-AUC and recall than realistic models.\n",
                "- This confirms the dataset authors' warning: `duration` is a **post-hoc variable** — it is only known *after* the call, at which point `y` is already determined.\n",
                "- **Including `duration` in production would constitute data leakage**, as future (unknown) information is used to predict the outcome.\n",
                "- The benchmark models exist purely to demonstrate this effect and should **never** be deployed.\n",
                "\n",
                "#### 2. LR vs SVM Comparison\n",
                "\n",
                "| Aspect | Logistic Regression | SVM (RBF) |\n",
                "|--------|--------------------|-----------|\n",
                "| **Speed** | Fast to train (~seconds) | Slower (~minutes on 41K rows) |\n",
                "| **Interpretability** | High — coefficients show feature importance | Low — kernel-based, black box |\n",
                "| **Handling Imbalance** | `class_weight='balanced'` works well | Same mechanism, similar effect |\n",
                "| **Non-linearity** | Cannot capture non-linear boundaries | RBF kernel captures non-linear patterns |\n",
                "| **Scalability** | Scales well to large datasets | O(n^2) to O(n^3) complexity — poor scaling |\n",
                "| **Deployment** | Lightweight model file | Larger model file (stores support vectors) |\n",
                "\n",
                "#### 3. Class Imbalance\n",
                "- The dataset is **heavily imbalanced** (~88.7% 'no' vs 11.3% 'yes').\n",
                "- Without `class_weight='balanced'`, models would predict 'no' for everything and still achieve ~89% accuracy.\n",
                "- We use `class_weight='balanced'` to penalise misclassification of the minority class.\n",
                "- **F1 and Recall** are more appropriate metrics than accuracy for this problem.\n",
                "\n",
                "#### 4. Deployment Model Selection\n",
                "- **Logistic Regression** is selected for deployment because:\n",
                "  1. Comparable or better performance to SVM on deployment features.\n",
                "  2. Significantly faster inference — critical for a web-based POC.\n",
                "  3. Smaller model file — easier to containerize.\n",
                "  4. Better interpretability — important for a banking use case.\n",
                "\n",
                "#### 5. Limitations\n",
                "- The deployment model uses only 7 client-profile features (no campaign or economic context).\n",
                "- This limits predictive power but satisfies the form requirement.\n",
                "- In a real-world system, campaign and economic features would be injected server-side."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<a id='5'></a>\n",
                "## 5. Model Export for Deployment"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save the deployment pipeline (LR with 7 features)\n",
                "model_path = 'model/lr_deployment_pipeline.joblib'\n",
                "joblib.dump(lr_deploy_pipe, model_path)\n",
                "model_size = os.path.getsize(model_path) / 1024\n",
                "print(f'Deployment model saved: {model_path} ({model_size:.1f} KB)')\n",
                "\n",
                "# Save feature names for validation\n",
                "feature_info = {\n",
                "    'features': DEPLOYMENT_FEATURES,\n",
                "    'categorical_values': {\n",
                "        col: sorted(df[col].unique().tolist())\n",
                "        for col in DEPLOYMENT_FEATURES if df[col].dtype == 'object'\n",
                "    }\n",
                "}\n",
                "with open('model/feature_info.json', 'w') as f:\n",
                "    json.dump(feature_info, f, indent=2)\n",
                "\n",
                "print('Feature info saved: model/feature_info.json')\n",
                "print('\\n=== Feature Info ===')\n",
                "print(json.dumps(feature_info, indent=2))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Quick sanity check — test the saved model\n",
                "loaded_pipe = joblib.load(model_path)\n",
                "\n",
                "# Simulate a form submission\n",
                "sample_input = pd.DataFrame([{\n",
                "    'age': 35,\n",
                "    'job': 'admin.',\n",
                "    'marital': 'married',\n",
                "    'education': 'university.degree',\n",
                "    'default': 'no',\n",
                "    'housing': 'yes',\n",
                "    'loan': 'no'\n",
                "}])\n",
                "\n",
                "prediction = loaded_pipe.predict(sample_input)[0]\n",
                "proba = loaded_pipe.predict_proba(sample_input)[0]\n",
                "\n",
                "result_label = 'YES' if prediction == 1 else 'NO'\n",
                "sample_dict = sample_input.to_dict(orient='records')[0]\n",
                "print(f'Sample input: {sample_dict}')\n",
                "print(f'Prediction:   {result_label}')\n",
                "print(f'Probability:  No={proba[0]:.3f}, Yes={proba[1]:.3f}')\n",
                "print('\\n Model loads and predicts correctly — ready for deployment!')"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}